{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interfere Jina Embedding Model Package from AWS Marketplace\n",
    "\n",
    "This notebook shows you how to deploy [jina-embedding-model](link) using Amazon SageMaker.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to [jina-embedding-model](link). If so, skip step: [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Real-time inference](#2.-Real-time-inference)\n",
    "   1. [Create an endpoint with static instances](#A.-Create-an-endpoint-with-static-instances)\n",
    "   2. [Create an endpoint that automatically scales](#B.-Create-an-endpoint-that-automatically-scales)\n",
    "   3. [Create an serverless endpoint](#C.-Create-an-serverless-endpoint)\n",
    "   4. [Perform real-time inference](#D.-Perform-real-time-inference)\n",
    "3. [Batch inference](#3.-Batch-inference)\n",
    "4. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    2. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade jina-sagemaker\n",
    "\n",
    "from jina_sagemaker import Client\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "# Specify the role if needed\n",
    "# role = None\n",
    "\n",
    "# Specify the model you want to use\n",
    "model_package_arn = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Real-time inference\n",
    "\n",
    "To learn about real-time inference capabilities in Amazon SageMaker, please refer to [Documentations](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Create an endpoint with static instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = Client(region_name=region)\n",
    "co.create_endpoint(arn=model_package_arn, role=role, endpoint_name=\"my-endpoint\", instance_type=\"ml.m5.xlarge\", n_instances=1)\n",
    "\n",
    "# If the endpoint is already created, you just need to connect to it\n",
    "# co.connect_to_endpoint(endpoint_name=\"my-endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Create an endpoint that automatically scales\n",
    "\n",
    "In this section, we configure an autoscaling endpoint that leverages step scaling, which scales a resource based on a set of scaling adjustments that vary based on the size of the alarm breach. For an in-depth understanding of step scaling, you can refer to the [Documentations](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html).\n",
    "\n",
    "When utilizing step scaling, it's your responsibility to configure the alarms that trigger the policy. Generally, you'll want to establish two alarms: one for triggering a step scale-in action and another for initiating a step scale-out action. Note that the upper and lower bounds specified in these policies are relative to the thresholds set in the corresponding alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = Client(region_name=region)\n",
    "co.create_endpoint(arn=model_package_arn, role=role, endpoint_name=\"my-autoscaling-endpoint\", instance_type=\"ml.m5.xlarge\", n_instances=2)\n",
    "\n",
    "# If the endpoint is already created, you just need to connect to it\n",
    "# co.connect_to_endpoint(endpoint_name=\"my-autoscaling_endpoint\")\n",
    "\n",
    "co.register_scalable_target(max_capacity=5, min_capacity=2)\n",
    "\n",
    "r = co.set_step_autoscaling(\n",
    "    policy_name=\"down\",\n",
    "    policy_configuration={\n",
    "        \"AdjustmentType\": \"ExactCapacity\",\n",
    "        \"StepAdjustments\": [\n",
    "            {\n",
    "                \"MetricIntervalUpperBound\": 0,\n",
    "                \"ScalingAdjustment\": 2,\n",
    "            }\n",
    "        ],\n",
    "        \"MetricAggregationType\": \"Average\",\n",
    "        \"Cooldown\": 10,\n",
    "    },\n",
    ")\n",
    "down_policy = r['PolicyARN']\n",
    "\n",
    "r = co.set_step_autoscaling(\n",
    "    policy_name=\"up\",\n",
    "    policy_configuration={\n",
    "        \"AdjustmentType\": \"ChangeInCapacity\",\n",
    "        \"StepAdjustments\": [\n",
    "            {\n",
    "                \"MetricIntervalLowerBound\": 0,\n",
    "                \"MetricIntervalUpperBound\": 10,\n",
    "                \"ScalingAdjustment\": 0,\n",
    "            },\n",
    "            {\n",
    "                \"MetricIntervalLowerBound\": 10,\n",
    "                \"MetricIntervalUpperBound\": 40,\n",
    "                \"ScalingAdjustment\": 3,\n",
    "            },\n",
    "            {\n",
    "                \"MetricIntervalLowerBound\": 40,\n",
    "                \"ScalingAdjustment\": 4,\n",
    "            },\n",
    "        ],\n",
    "        \"MetricAggregationType\": \"Average\",\n",
    "        \"Cooldown\": 10,\n",
    "    },\n",
    ")\n",
    "up_policy = r['PolicyARN']\n",
    "\n",
    "co.set_metric_alarm(policy_arn=down_policy, \n",
    "    AlarmName=f\"step_scaling_policy_alarm_down\",\n",
    "    MetricName=\"CPUUtilization\",\n",
    "    Namespace=\"/aws/sagemaker/Endpoints\",\n",
    "    Statistic=\"Average\",\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=30,\n",
    "    ComparisonOperator=\"LessThanOrEqualToThreshold\",\n",
    "    TreatMissingData=\"missing\",\n",
    "    Period=60,\n",
    "    Unit=\"Percent\",\n",
    ")\n",
    "\n",
    "co.set_metric_alarm(policy_arn=up_policy,\n",
    "    AlarmName=f\"step_scaling_policy_alarm_up\",\n",
    "    MetricName=\"CPUUtilization\",\n",
    "    Namespace=\"/aws/sagemaker/Endpoints\",\n",
    "    Statistic=\"Average\",\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=60,\n",
    "    ComparisonOperator=\"GreaterThanThreshold\",\n",
    "    TreatMissingData=\"missing\",\n",
    "    Period=10,\n",
    "    Unit=\"Percent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Create an serverless endpoint\n",
    "\n",
    "To learn about serverless inference in Amazon SageMaker, please refer to [Documentations](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "\n",
    "sls_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=6144,\n",
    "    max_concurrency=20,\n",
    "    provisioned_concurrency=10\n",
    ")\n",
    "\n",
    "co.create_endpoint(arn=model_package_arn, role=role, endpoint_name=\"my-sls-endpoint\", sls_config=sls_config)\n",
    "\n",
    "# If the endpoint is already created, you just need to connect to it\n",
    "# co.connect_to_endpoint(endpoint_name=\"my-sls_endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03703858703374863, 0.053599365055561066, 0.012735798954963684]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "result = co.embed(model='jina-embedding-t-en-v1', texts=[\"how is the weather today\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Batch inference\n",
    "\n",
    "To learn about batch transform capabilities in Amazon SageMaker, please refer to [Documentations](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co.create_transform_job(\n",
    "    arn=model_package_arn,\n",
    "    role=role,\n",
    "    n_instances=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    input_path=\"s3://path/to/input/data\",\n",
    "    output_path=\"s3://path/to/output\",\n",
    "    content_type=\"application/csv\",\n",
    "    split_type=\"Line\",\n",
    "    strategy=\"MultiRecord\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co.delete_endpoint()\n",
    "co.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Unsubscribe to the listing (optional)\n",
    "\n",
    "If you would like to unsubscribe to the model package, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
